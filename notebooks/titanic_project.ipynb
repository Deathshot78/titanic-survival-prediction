{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7f36ba3",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction: MLP with PyTorch Lightning & Optuna\n",
    "\n",
    "This notebook contains an end-to-end workflow for the Kaggle Titanic competition. The main steps are:,\n",
    "1.  **Preprocessing**: Clean the data and engineer new features.,\n",
    "2.  **Data Module**: Set up a PyTorch Lightning `DataModule` to handle datasets and dataloaders.,\n",
    "3.  **Model Definition**: Define the MLP architecture using PyTorch Lightning.,\n",
    "4.  **Hyperparameter Tuning**: Use Optuna to find the best hyperparameters for the model.,\n",
    "5.  **Final Training & Prediction**: Train the best model on all data and generate the submission file.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d3c628",
   "metadata": {},
   "source": [
    "##    Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "773bbb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import lightgbm as lgb\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import re\n",
    "import warnings\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Suppress unnecessary warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eae9dc",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "\n",
    "We define a single function to handle all data preprocessing steps consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fee36ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_path='data/train.csv', test_path='data/test.csv'):\n",
    "    \"\"\"\n",
    "    Loads, cleans, and feature-engineers the Titanic dataset.\n",
    "\n",
    "    This function handles:\n",
    "    - Loading train and test CSVs.\n",
    "    - Engineering features like Title, FamilySize, IsAlone, and Deck.\n",
    "    - Imputing missing values for Age, Fare, and Embarked.\n",
    "    - Returning separate, processed dataframes for training and prediction,\n",
    "      along with metadata useful for modeling.\n",
    "\n",
    "    Args:\n",
    "        train_path (str): The file path for the training data.\n",
    "        test_path (str): The file path for the test data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - X_full (pd.DataFrame): The full processed training feature set.\n",
    "            - y_full (pd.Series): The full training target variable.\n",
    "            - X_predict (pd.DataFrame): The full processed test feature set.\n",
    "            - test_passenger_ids (pd.Series): The PassengerIds for the test set.\n",
    "            - numerical_features (list): A list of numerical column names.\n",
    "            - categorical_features (list): A list of categorical column names.\n",
    "    \"\"\"\n",
    "    print(\"--- Loading and Preprocessing Data ---\")\n",
    "    try:\n",
    "        train_df_orig = pd.read_csv(train_path)\n",
    "        test_df_orig = pd.read_csv(test_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Could not find {e.filename}. Please ensure train.csv and test.csv are in the correct directory.\")\n",
    "        raise\n",
    "\n",
    "    test_passenger_ids = test_df_orig['PassengerId']\n",
    "    y_full = train_df_orig['Survived'].copy()\n",
    "    \n",
    "    combined_df = pd.concat([train_df_orig.drop('Survived', axis=1), test_df_orig], ignore_index=True)\n",
    "\n",
    "    # Feature Engineering\n",
    "    def engineer_features(df):\n",
    "        df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\\\.', expand=False)\n",
    "        common_titles = ['Mr', 'Miss', 'Mrs', 'Master']\n",
    "        df['Title'] = df['Title'].apply(lambda x: x if x in common_titles else 'Other')\n",
    "        df['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n",
    "        df['IsAlone'] = (df['FamilySize'] == 1).astype(int)\n",
    "        df['Deck'] = df['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'U')\n",
    "        return df\n",
    "\n",
    "    combined_df = engineer_features(combined_df)\n",
    "\n",
    "    # Imputation \n",
    "    combined_df['Embarked'].fillna(combined_df['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "    # First, calculate the grouped medians. This might have NaNs for groups where all ages are missing.\n",
    "    age_median_map = combined_df.groupby(['Pclass', 'Title'])['Age'].median()\n",
    "    # Then, calculate a global median to use as a fallback.\n",
    "    global_age_median = combined_df['Age'].median()\n",
    "    # Fill any NaNs in the median map itself with the global median.\n",
    "    age_median_map.fillna(global_age_median, inplace=True)\n",
    "    # Now, we can safely apply this robust map to fill NaNs in the main dataframe.\n",
    "    combined_df['Age'] = combined_df.apply(\n",
    "        lambda row: age_median_map.loc[row['Pclass'], row['Title']] if pd.isnull(row['Age']) else row['Age'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Robust Fare Imputation\n",
    "    combined_df['Fare'] = combined_df.groupby('Pclass')['Fare'].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "    # Final Feature Selection \n",
    "    numerical_features = ['Age', 'Fare', 'FamilySize', 'IsAlone']\n",
    "    categorical_features = ['Pclass', 'Sex', 'Embarked', 'Title', 'Deck']\n",
    "    \n",
    "    combined_df_processed = combined_df[numerical_features + categorical_features]\n",
    "\n",
    "    # Final check for any remaining NaNs\n",
    "    if combined_df_processed.isnull().sum().sum() > 0:\n",
    "        print(\"Error: NaNs still present after preprocessing!\")\n",
    "        print(combined_df_processed.isnull().sum())\n",
    "        raise ValueError(\"Preprocessing failed, NaNs remain in the data.\")\n",
    "    \n",
    "    # Split back into final training and prediction sets \n",
    "    X_full = combined_df_processed.iloc[:len(train_df_orig)]\n",
    "    X_predict = combined_df_processed.iloc[len(train_df_orig):]\n",
    "\n",
    "    print(\"Preprocessing complete.\")\n",
    "    \n",
    "    return X_full, y_full, X_predict, test_passenger_ids, numerical_features, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d8ef87c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running preprocessing script as a standalone test...\n",
      "--- Loading and Preprocessing Data ---\n",
      "Preprocessing complete.\n",
      "\n",
      "Shape of processed training features (X_full): (891, 9)\n",
      "Shape of training labels (y_full): (891,)\n",
      "Shape of processed test features (X_predict): (418, 9)\n",
      "\n",
      "First 5 rows of processed training data:\n",
      "    Age     Fare  FamilySize  IsAlone  Pclass     Sex Embarked Title Deck\n",
      "0  22.0   7.2500           2        0       3    male        S    Mr    U\n",
      "1  38.0  71.2833           2        0       1  female        C   Mrs    C\n",
      "2  26.0   7.9250           1        1       3  female        S  Miss    U\n",
      "3  35.0  53.1000           2        0       1  female        S   Mrs    C\n",
      "4  35.0   8.0500           1        1       3    male        S    Mr    U\n"
     ]
    }
   ],
   "source": [
    "print(\"Running preprocessing script as a standalone test...\")\n",
    "X_train, y_train, X_test, _, _, _ = preprocess_data()\n",
    "print(\"\\nShape of processed training features (X_full):\", X_train.shape)\n",
    "print(\"Shape of training labels (y_full):\", y_train.shape)\n",
    "print(\"Shape of processed test features (X_predict):\", X_test.shape)\n",
    "print(\"\\nFirst 5 rows of processed training data:\")\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11ef911",
   "metadata": {},
   "source": [
    "## 3. PyTorch Lightning DataModule\n",
    "\n",
    "This module encapsulates all data handling logic: loading, splitting, scaling, and creating `DataLoaders`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitanicDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32),\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32).unsqueeze(1) if y is not None else None,\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is not None:\n",
    "            return self.X[idx], self.y[idx]\n",
    "        else:\n",
    "            return self.X[idx]\n",
    "\n",
    "class TitanicDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, X_full, y_full, X_predict, batch_size=64, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['X_full', 'y_full', 'X_predict'])\n",
    "        self.X_full = X_full\n",
    "        self.y_full = y_full\n",
    "        self.X_predict = X_predict\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Scale numerical features based on the training set\n",
    "        self.X_full_scaled = self.scaler.fit_transform(self.X_full)\n",
    "        self.X_predict_scaled = self.scaler.transform(self.X_predict)\n",
    "\n",
    "        # Split data for training, validation, and testing\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(self.X_full_scaled, self.y_full, test_size=0.3, random_state=42, stratify=self.y_full)\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "        self.train_ds = TitanicDataset(pd.DataFrame(X_train), y_train)\n",
    "        self.val_ds = TitanicDataset(pd.DataFrame(X_val), y_val)\n",
    "        self.test_ds = TitanicDataset(pd.DataFrame(X_test), y_test)\n",
    "        self.predict_ds = TitanicDataset(pd.DataFrame(self.X_predict_scaled))\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size=self.hparams.batch_size, shuffle=True, num_workers=self.hparams.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers)\n",
    "    \n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.predict_ds, batch_size=self.hparams.batch_size, num_workers=self.hparams.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c251d58",
   "metadata": {},
   "source": [
    "## 4. PyTorch Lightning Model (FT Transformer)\n",
    "\n",
    "Here we define our FT Transformer model. It's a `LightningModule`, which organizes the model architecture, training logic, and optimizer configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b62bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTokenizer(nn.Module):\n",
    "    \"\"\"\n",
    "    Tokenizes numerical and categorical features into a sequence of embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_numerical_features, cat_cardinalities, embed_dim):\n",
    "        super().__init__()\n",
    "        self.num_linears = nn.ModuleList([nn.Linear(1, embed_dim) for _ in range(num_numerical_features)])\n",
    "        self.cat_embeddings = nn.ModuleList([nn.Embedding(num_classes, embed_dim) for num_classes in cat_cardinalities])\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        batch_size = x_num.shape[0]\n",
    "        \n",
    "        # Tokenize numerical features\n",
    "        num_token_list = []\n",
    "        x_num_unsqueezed = x_num.unsqueeze(-1)\n",
    "        for i, linear_layer in enumerate(self.num_linears):\n",
    "            num_token_list.append(linear_layer(x_num_unsqueezed[:, i, :]))\n",
    "        \n",
    "        # Tokenize categorical features\n",
    "        cat_token_list = []\n",
    "        x_cat_long = x_cat.long()\n",
    "        for i, embedding_layer in enumerate(self.cat_embeddings):\n",
    "            cat_token_list.append(embedding_layer(x_cat_long[:, i]))\n",
    "        \n",
    "        num_tokens = torch.stack(num_token_list, dim=1)\n",
    "        cat_tokens = torch.stack(cat_token_list, dim=1)\n",
    "        \n",
    "        # Concatenate all feature tokens\n",
    "        feature_tokens = torch.cat([num_tokens, cat_tokens], dim=1)\n",
    "        \n",
    "        # Prepend the [CLS] token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        tokens = torch.cat([cls_tokens, feature_tokens], dim=1)\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Transformer block with Multi-Head Attention and a Feed-Forward Network.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        attn_out, _ = self.attn(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attn_out))\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_out))\n",
    "        return x\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A stack of TransformerBlocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class PredictionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    The final prediction head that takes the [CLS] token output.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(embed_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # Use the output of the CLS token (the first token) for prediction\n",
    "        cls_token_output = x[:, 0]\n",
    "        return self.fc(cls_token_output)\n",
    "\n",
    "# Main LightningModule for the FT-Transformer \n",
    "class FTTransformerFromScratch(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    The main PyTorch Lightning module that combines all the building blocks\n",
    "    and defines the training, validation, and prediction logic.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_numerical, cat_cardinalities, embed_dim, num_heads, ff_dim, num_layers, lr, weight_decay, dropout):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Build the model from the blocks\n",
    "        self.tokenizer = FeatureTokenizer(num_numerical, cat_cardinalities, embed_dim)\n",
    "        self.encoder = TransformerEncoder(embed_dim, num_heads, ff_dim, num_layers, dropout)\n",
    "        self.head = PredictionHead(embed_dim, 1) # Output dim is 1 for binary classification\n",
    "        \n",
    "        # Define loss function and metrics\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"binary\")\n",
    "\n",
    "    def forward(self, x_num, x_cat):\n",
    "        x = self.tokenizer(x_num, x_cat)\n",
    "        x = self.encoder(x)\n",
    "        output = self.head(x)\n",
    "        return output\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_num, x_cat, y = batch\n",
    "        logits = self(x_num, x_cat)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x_num, x_cat, y = batch\n",
    "        logits = self(x_num, x_cat)\n",
    "        loss = self.loss_fn(logits, y)\n",
    "        self.val_acc(logits, y)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True)\n",
    "        self.log(\"val_acc\", self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x_num, x_cat = batch\n",
    "        logits = self(x_num, x_cat)\n",
    "        preds = torch.round(torch.sigmoid(logits))\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36c66f0",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning with Optuna\n",
    "\n",
    "Here we define the optuna objective functions for all 3 models (FT Transformer, LGBM, Simple Logistic Regression) to find the best Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d3340dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_ft_transformer(trial, X_train, y_train, X_val, y_val, numerical_features, categorical_features):\n",
    "    \"\"\"Objective function for tuning the from-scratch FT-Transformer.\"\"\"\n",
    "    X_train_ft, X_val_ft = X_train.copy(), X_val.copy()\n",
    "    scaler = StandardScaler()\n",
    "    X_train_ft[numerical_features] = scaler.fit_transform(X_train_ft[numerical_features])\n",
    "    X_val_ft[numerical_features] = scaler.transform(X_val_ft[numerical_features])\n",
    "    \n",
    "    cat_maps = {col: X_train_ft[col].astype('category').cat.categories for col in categorical_features}\n",
    "    cardinalities = [len(cat_maps[col]) for col in categorical_features]\n",
    "    \n",
    "    for col in categorical_features:\n",
    "        X_train_ft[col] = pd.Categorical(X_train_ft[col], categories=cat_maps[col]).codes\n",
    "        X_val_ft[col] = pd.Categorical(X_val_ft[col], categories=cat_maps[col]).codes\n",
    "\n",
    "    train_ds = TitanicFTDataset(X_train_ft[numerical_features], X_train_ft[categorical_features], y_train)\n",
    "    val_ds = TitanicFTDataset(X_val_ft[numerical_features], X_val_ft[categorical_features], y_val)\n",
    "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=32)\n",
    "\n",
    "    # Hyperparameter suggestions for the FT-Transformer\n",
    "    embed_dim = trial.suggest_categorical(\"embed_dim\", [128, 192, 256])\n",
    "    num_heads = trial.suggest_categorical(\"num_heads\", [4, 8])\n",
    "    if embed_dim % num_heads != 0: raise optuna.exceptions.TrialPruned(\"embed_dim must be divisible by num_heads.\")\n",
    "    \n",
    "    ff_dim = embed_dim * trial.suggest_categorical(\"ff_dim_factor\", [2, 4])\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.4)\n",
    "\n",
    "    model = FTTransformerFromScratch(\n",
    "        num_numerical=len(numerical_features), \n",
    "        cat_cardinalities=cardinalities, \n",
    "        embed_dim=embed_dim, \n",
    "        num_heads=num_heads, \n",
    "        ff_dim=ff_dim, \n",
    "        num_layers=num_layers, \n",
    "        lr=lr, \n",
    "        weight_decay=weight_decay, \n",
    "        dropout=dropout\n",
    "    )\n",
    "    trainer = pl.Trainer(max_epochs=30, accelerator=\"auto\", callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5)], logger=False, enable_progress_bar=False, enable_model_summary=False)\n",
    "    \n",
    "    try:\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        return trainer.callback_metrics.get(\"val_acc\", 0.0).item()\n",
    "    except Exception as e:\n",
    "        print(f\"Trial failed for FT-Transformer with error: {e}\")\n",
    "        return 0.0 # Return a low score to Optuna\n",
    "\n",
    "\n",
    "def objective_lgbm(trial, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Objective function for tuning LightGBM.\"\"\"\n",
    "    params = {\n",
    "        'objective': 'binary', 'verbosity': -1,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 2000), \n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 300), \n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True), \n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0), \n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100)\n",
    "    }\n",
    "    \n",
    "    X_train_lgbm, X_val_lgbm = X_train.copy(), X_val.copy()\n",
    "    for col in X_train_lgbm.select_dtypes(include=['object', 'category']).columns:\n",
    "        X_train_lgbm[col] = X_train_lgbm[col].astype('category')\n",
    "        X_val_lgbm[col] = X_val_lgbm[col].astype('category')\n",
    "    \n",
    "    model = lgb.LGBMClassifier(**params)\n",
    "    model.fit(X_train_lgbm, y_train, \n",
    "              eval_set=[(X_val_lgbm, y_val)], \n",
    "              eval_metric='accuracy', \n",
    "              callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "    return accuracy_score(y_val, model.predict(X_val_lgbm))\n",
    "\n",
    "\n",
    "def objective_logreg(trial, X_train, y_train, X_val, y_val, numerical_features, categorical_features):\n",
    "    \"\"\"Objective function for tuning Logistic Regression.\"\"\"\n",
    "    C = trial.suggest_float(\"C\", 1e-4, 1e2, log=True)\n",
    "    solver = trial.suggest_categorical(\"solver\", [\"liblinear\", \"lbfgs\", \"saga\"])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ])\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(random_state=42, C=C, solver=solver, max_iter=2000))\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    return accuracy_score(y_val, pipeline.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c053395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and Preprocessing Data ---\n",
      "Preprocessing complete.\n",
      "\n",
      "--- Tuning FT-Transformer ---\n",
      "Best FT-Transformer Val Accuracy: 0.8380\n",
      "\n",
      "--- Tuning LightGBM ---\n",
      "Best LightGBM Val Accuracy: 0.8547\n",
      "\n",
      "--- Tuning Logistic Regression ---\n",
      "Best Logistic Regression Val Accuracy: 0.8547\n"
     ]
    }
   ],
   "source": [
    "# Get preprocessed data and split it for validation\n",
    "X_full, y_full, _, _, numerical_features, categorical_features = preprocess_data()\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_full, y_full, test_size=0.2, random_state=42, stratify=y_full)\n",
    "\n",
    "all_best_params = {}\n",
    "\n",
    "# Tune All Models \n",
    "print(\"\\n--- Tuning FT-Transformer ---\")\n",
    "study_ft = optuna.create_study(direction=\"maximize\")\n",
    "study_ft.optimize(lambda t: objective_ft_transformer(t, X_train, y_train, X_val, y_val, numerical_features, categorical_features), n_trials=30)\n",
    "all_best_params['ft_transformer'] = study_ft.best_params\n",
    "print(f\"Best FT-Transformer Val Accuracy: {study_ft.best_value:.4f}\")\n",
    "\n",
    "print(\"\\n--- Tuning LightGBM ---\")\n",
    "study_lgbm = optuna.create_study(direction=\"maximize\")\n",
    "study_lgbm.optimize(lambda t: objective_lgbm(t, X_train, y_train, X_val, y_val), n_trials=30)\n",
    "all_best_params['lightgbm'] = study_lgbm.best_params\n",
    "print(f\"Best LightGBM Val Accuracy: {study_lgbm.best_value:.4f}\")\n",
    "\n",
    "print(\"\\n--- Tuning Logistic Regression ---\")\n",
    "study_logreg = optuna.create_study(direction=\"maximize\")\n",
    "study_logreg.optimize(lambda t: objective_logreg(t, X_train, y_train, X_val, y_val, numerical_features, categorical_features), n_trials=30)\n",
    "all_best_params['logistic_regression'] = study_logreg.best_params\n",
    "print(f\"Best Logistic Regression Val Accuracy: {study_logreg.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1b62209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------------------\n",
      "Hyperparameter tuning complete for all models.\n",
      "Best parameters saved to 'best_hyperparameters.json'\n",
      "----------------------------------------------------\n",
      "\n",
      "Best Parameters Found:\n",
      "{\n",
      "    \"ft_transformer\": {\n",
      "        \"embed_dim\": 192,\n",
      "        \"num_heads\": 4,\n",
      "        \"ff_dim_factor\": 4,\n",
      "        \"num_layers\": 3,\n",
      "        \"lr\": 6.633442318929932e-05,\n",
      "        \"weight_decay\": 1.969683126228165e-05,\n",
      "        \"dropout\": 0.2356785069572193\n",
      "    },\n",
      "    \"lightgbm\": {\n",
      "        \"n_estimators\": 642,\n",
      "        \"learning_rate\": 0.04515157839370836,\n",
      "        \"num_leaves\": 36,\n",
      "        \"max_depth\": 6,\n",
      "        \"reg_alpha\": 2.722043594358596e-07,\n",
      "        \"reg_lambda\": 2.0379171849455357e-05,\n",
      "        \"colsample_bytree\": 0.6570997858667309,\n",
      "        \"subsample\": 0.9980314167105858,\n",
      "        \"min_child_samples\": 92\n",
      "    },\n",
      "    \"logistic_regression\": {\n",
      "        \"C\": 2.4633756846663513,\n",
      "        \"solver\": \"saga\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Save Best Hyperparameters to a file\n",
    "output_path = 'logs'\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(output_path ,'best_hyperparameters.json'), 'w') as f:\n",
    "    json.dump(all_best_params, f, indent=4)\n",
    "    \n",
    "print(\"\\n----------------------------------------------------\")\n",
    "print(\"Hyperparameter tuning complete for all models.\")\n",
    "print(\"Best parameters saved to 'best_hyperparameters.json'\")\n",
    "print(\"----------------------------------------------------\")\n",
    "print(\"\\nBest Parameters Found:\")\n",
    "print(json.dumps(all_best_params, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5297faa",
   "metadata": {},
   "source": [
    "## 6. Training the models with the hyperparameters found by optuna "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71063a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 'best_hyperparameters.json'\n",
      "--- Loading and Preprocessing Data ---\n",
      "Preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "pl.seed_everything(42)\n",
    "\n",
    "# Load Data and Best Hyperparameters\n",
    "try:\n",
    "    with open('logs/best_hyperparameters.json', 'r') as f:\n",
    "        all_best_params = json.load(f)\n",
    "    print(\"Successfully loaded 'best_hyperparameters.json'\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'best_hyperparameters.json' not found.\")\n",
    "    print(\"Please run the 'objective_optuna.py' script first to generate the hyperparameters.\")\n",
    "    exit()\n",
    "\n",
    "X_full, y_full, X_predict, test_passenger_ids, numerical_features, categorical_features = preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "738ac2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training Final Models on Full Data and Generating Submissions ---\n",
      "\n",
      "1. Training and Predicting with final FT-Transformer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22906bb2c3254abfaa920b55244ae1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac436361b8547de8ca8f789dde9fa31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FT-Transformer submission file created.\n",
      "\n",
      "2. Training and Predicting with final LightGBM...\n",
      "LightGBM submission file created.\n",
      "\n",
      "3. Training and Predicting with final Logistic Regression...\n",
      "Logistic Regression submission file created.\n",
      "\n",
      "--- Creating Final Ensemble Submission ---\n",
      "\n",
      "----------------------------------------\n",
      "All submission files created successfully!\n",
      "You can now upload them to the Kaggle competition.\n",
      "----------------------------------------\n",
      "Ensemble submission head:\n",
      "   PassengerId  Survived\n",
      "0          892         0\n",
      "1          893         0\n",
      "2          894         0\n",
      "3          895         0\n",
      "4          896         1\n"
     ]
    }
   ],
   "source": [
    " # --- Train Final Models and Predict ---\n",
    "print(\"\\n--- Training Final Models on Full Data and Generating Submissions ---\")\n",
    "\n",
    "# --- Model 1: FT-Transformer ---\n",
    "print(\"\\n1. Training and Predicting with final FT-Transformer...\")\n",
    "best_params_ft = all_best_params['ft_transformer']\n",
    "\n",
    "# Data preparation specific to FT-Transformer\n",
    "X_train_ft_final, X_predict_ft_final = X_full.copy(), X_predict.copy()\n",
    "scaler_ft = StandardScaler()\n",
    "X_train_ft_final[numerical_features] = scaler_ft.fit_transform(X_train_ft_final[numerical_features])\n",
    "X_predict_ft_final[numerical_features] = scaler_ft.transform(X_predict_ft_final[numerical_features])\n",
    "\n",
    "cat_maps = {col: X_train_ft_final[col].astype('category').cat.categories for col in categorical_features}\n",
    "cardinalities = [len(cat_maps[col]) for col in categorical_features]\n",
    "\n",
    "for col in categorical_features:\n",
    "    X_train_ft_final[col] = pd.Categorical(X_train_ft_final[col], categories=cat_maps[col]).codes\n",
    "    X_predict_ft_final[col] = pd.Categorical(X_predict_ft_final[col], categories=cat_maps[col]).codes\n",
    "    # Handle any categories in predict set not seen in training\n",
    "    if (X_predict_ft_final[col] == -1).any():\n",
    "        mode_code = cat_maps[col].get_loc(X_full[col].mode()[0])\n",
    "        X_predict_ft_final[col].replace(-1, mode_code, inplace=True)\n",
    "        \n",
    "final_train_ds = TitanicFTDataset(X_train_ft_final[numerical_features], X_train_ft_final[categorical_features], y_full)\n",
    "predict_ds = TitanicFTDataset(X_predict_ft_final[numerical_features], X_predict_ft_final[categorical_features])\n",
    "final_train_loader = DataLoader(final_train_ds, batch_size=32, shuffle=True)\n",
    "predict_loader = DataLoader(predict_ds, batch_size=32)\n",
    "\n",
    "# Instantiate and train the final model\n",
    "final_model_ft = FTTransformerFromScratch(\n",
    "    num_numerical=len(numerical_features), \n",
    "    cat_cardinalities=cardinalities,\n",
    "    embed_dim=best_params_ft['embed_dim'], \n",
    "    num_heads=best_params_ft['num_heads'],\n",
    "    ff_dim=best_params_ft['embed_dim'] * best_params_ft['ff_dim_factor'],\n",
    "    num_layers=best_params_ft['num_layers'], \n",
    "    lr=best_params_ft['lr'], \n",
    "    weight_decay=best_params_ft['weight_decay'], \n",
    "    dropout=best_params_ft['dropout']\n",
    ")\n",
    "final_trainer_ft = pl.Trainer(max_epochs=30, accelerator=\"auto\", logger=False, enable_progress_bar=True, enable_model_summary=False)\n",
    "final_trainer_ft.fit(final_model_ft, final_train_loader)\n",
    "\n",
    "# Predict and save\n",
    "preds_ft_final = torch.cat(final_trainer_ft.predict(final_model_ft, predict_loader)).flatten().cpu().numpy().astype(int)\n",
    "pd.DataFrame({'PassengerId': test_passenger_ids, 'Survived': preds_ft_final}).to_csv('data/submission_ft_transformer_tuned.csv', index=False)\n",
    "print(\"FT-Transformer submission file created.\")\n",
    "\n",
    "# --- Model 2: LightGBM ---\n",
    "print(\"\\n2. Training and Predicting with final LightGBM...\")\n",
    "best_params_lgbm = all_best_params['lightgbm']\n",
    "\n",
    "X_full_lgbm, X_predict_lgbm = X_full.copy(), X_predict.copy()\n",
    "for col in categorical_features:\n",
    "    X_full_lgbm[col] = X_full_lgbm[col].astype('category')\n",
    "    X_predict_lgbm[col] = X_predict_lgbm[col].astype('category')\n",
    "    \n",
    "final_lgbm = lgb.LGBMClassifier(objective='binary', **best_params_lgbm)\n",
    "final_lgbm.fit(X_full_lgbm, y_full)\n",
    "preds_lgbm = final_lgbm.predict(X_predict_lgbm)\n",
    "pd.DataFrame({'PassengerId': test_passenger_ids, 'Survived': preds_lgbm}).to_csv('data/submission_lgbm_tuned.csv', index=False)\n",
    "print(\"LightGBM submission file created.\")\n",
    "\n",
    "# --- Model 3: Logistic Regression ---\n",
    "print(\"\\n3. Training and Predicting with final Logistic Regression...\")\n",
    "best_params_logreg = all_best_params['logistic_regression']\n",
    "\n",
    "preprocessor_final = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), numerical_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "])\n",
    "\n",
    "final_logreg = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_final),\n",
    "    ('classifier', LogisticRegression(random_state=42, max_iter=2000, **best_params_logreg))\n",
    "])\n",
    "final_logreg.fit(X_full, y_full)\n",
    "preds_lr = final_logreg.predict(X_predict)\n",
    "pd.DataFrame({'PassengerId': test_passenger_ids, 'Survived': preds_lr}).to_csv('data/submission_logreg_tuned.csv', index=False)\n",
    "print(\"Logistic Regression submission file created.\")\n",
    "\n",
    "# --- Final Ensemble ---\n",
    "print(\"\\n--- Creating Final Ensemble Submission ---\")\n",
    "stacked_preds = np.vstack([preds_ft_final, preds_lgbm, preds_lr]).T\n",
    "from scipy.stats import mode\n",
    "ensemble_preds, _ = mode(stacked_preds, axis=1)\n",
    "\n",
    "submission_df = pd.DataFrame({\n",
    "    'PassengerId': test_passenger_ids,\n",
    "    'Survived': ensemble_preds.flatten()\n",
    "})\n",
    "submission_df.to_csv('data/ensemble_submission_tuned.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 40)\n",
    "print(\"All submission files created successfully!\")\n",
    "print(\"You can now upload them to the Kaggle competition.\")\n",
    "print(\"-\" * 40)\n",
    "print(\"Ensemble submission head:\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
